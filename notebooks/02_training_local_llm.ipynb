{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071da243",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23034c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# Uncomment the lines below if you haven't installed these packages\n",
    "\n",
    "# Core packages\n",
    "# !pip install transformers datasets accelerate bitsandbytes\n",
    "\n",
    "# For LoRA fine-tuning\n",
    "# !pip install peft trl\n",
    "\n",
    "# For fast fine-tuning (recommended!)\n",
    "# !pip install unsloth\n",
    "\n",
    "# Database connection\n",
    "# !pip install psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path('..') / '.env')\n",
    "\n",
    "print(\"Environment loaded!\")\n",
    "print(f\"HF Token available: {'HF_TOKEN' in os.environ or 'HUGGINGFACE_TOKEN' in os.environ}\")\n",
    "print(f\"Neon DB available: {'NEON_DATABASE_URL' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af82b30",
   "metadata": {},
   "source": [
    "## 2. Export Training Data from Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32899549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "def get_training_data():\n",
    "    \"\"\"Fetch training examples from Neon database.\"\"\"\n",
    "    conn = psycopg2.connect(os.environ['NEON_DATABASE_URL'])\n",
    "    cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "    \n",
    "    # Get summary first\n",
    "    cur.execute(\"SELECT * FROM training_data_summary\")\n",
    "    summary = cur.fetchone()\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Training Data Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    if summary:\n",
    "        print(f\"Total interactions: {summary.get('total_interactions', 0)}\")\n",
    "        print(f\"Approved: {summary.get('approved_count', 0)}\")\n",
    "        print(f\"Rejected: {summary.get('rejected_count', 0)}\")\n",
    "        print(f\"Corrected: {summary.get('corrected_count', 0)} â† GOLD DATA!\")\n",
    "        print(f\"Training examples: {summary.get('training_examples', 0)}\")\n",
    "    else:\n",
    "        print(\"No training data yet. Run the curator agent with HITL!\")\n",
    "    \n",
    "    # Get training examples\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT instruction, input, output, quality_score\n",
    "        FROM training_examples\n",
    "        WHERE quality_score >= 0.7\n",
    "        ORDER BY created_at DESC\n",
    "    \"\"\")\n",
    "    examples = cur.fetchall()\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return examples\n",
    "\n",
    "training_examples = get_training_data()\n",
    "print(f\"\\nHigh-quality training examples: {len(training_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few examples\n",
    "if training_examples:\n",
    "    print(\"Sample Training Example:\")\n",
    "    print(\"-\" * 50)\n",
    "    example = training_examples[0]\n",
    "    print(f\"Instruction: {example['instruction'][:200]}...\")\n",
    "    print(f\"Input: {example['input'][:200] if example['input'] else 'N/A'}...\")\n",
    "    print(f\"Output: {example['output'][:200]}...\")\n",
    "    print(f\"Quality: {example['quality_score']}\")\n",
    "else:\n",
    "    print(\"No training examples yet!\")\n",
    "    print(\"\\nTo generate training data:\")\n",
    "    print(\"1. Run: python curator-agent/run_with_approval.py\")\n",
    "    print(\"2. When prompted, choose [e]dit to provide corrections\")\n",
    "    print(\"3. Your corrections become high-quality training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410415d0",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_for_training(examples):\n",
    "    \"\"\"Convert to Alpaca-style format for fine-tuning.\"\"\"\n",
    "    formatted = []\n",
    "    for ex in examples:\n",
    "        # Alpaca format with input field\n",
    "        if ex['input']:\n",
    "            text = f\"\"\"### Instruction:\n",
    "{ex['instruction']}\n",
    "\n",
    "### Input:\n",
    "{ex['input']}\n",
    "\n",
    "### Response:\n",
    "{ex['output']}\"\"\"\n",
    "        else:\n",
    "            text = f\"\"\"### Instruction:\n",
    "{ex['instruction']}\n",
    "\n",
    "### Response:\n",
    "{ex['output']}\"\"\"\n",
    "        \n",
    "        formatted.append({\"text\": text})\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Create dataset\n",
    "if training_examples:\n",
    "    formatted_data = format_for_training(training_examples)\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    print(f\"Dataset created with {len(dataset)} examples\")\n",
    "    print(\"\\nSample formatted text:\")\n",
    "    print(dataset[0]['text'][:500] + \"...\")\n",
    "else:\n",
    "    print(\"Need training examples first!\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset locally for backup\n",
    "output_dir = Path('../data/training')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if dataset:\n",
    "    # Save as JSONL\n",
    "    jsonl_path = output_dir / 'proves_training_data.jsonl'\n",
    "    with open(jsonl_path, 'w') as f:\n",
    "        for example in formatted_data:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "    print(f\"Saved to {jsonl_path}\")\n",
    "    \n",
    "    # Also save as HF dataset\n",
    "    dataset.save_to_disk(str(output_dir / 'proves_dataset'))\n",
    "    print(f\"Saved HF dataset to {output_dir / 'proves_dataset'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410c066",
   "metadata": {},
   "source": [
    "## 4. Fine-Tune with Unsloth (Fast + Memory Efficient)\n",
    "\n",
    "Unsloth provides 2x faster fine-tuning with 50% less memory.\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- 1B model: 4GB VRAM (RTX 3060 or better)\n",
    "- 7B model: 16GB VRAM (RTX 4090 or A100)\n",
    "- Or use Google Colab with T4 GPU (free tier works for small models!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Recommend model based on VRAM\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if vram_gb >= 16:\n",
    "        print(\"\\nâœ… Recommended: unsloth/mistral-7b-instruct-v0.3\")\n",
    "    elif vram_gb >= 8:\n",
    "        print(\"\\nâœ… Recommended: HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Recommended: unsloth/Llama-3.2-1B-Instruct\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Use Google Colab (free T4 GPU)\")\n",
    "    print(\"2. Use CPU (very slow, not recommended)\")\n",
    "    print(\"3. Rent GPU from vast.ai, runpod.io, or lambda labs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53517537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning code (requires GPU)\n",
    "# Uncomment when ready to train\n",
    "\n",
    "'''\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",  # Change based on your VRAM\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # QLoRA - uses less VRAM\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=100,  # Increase for more training\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "print(\"Fine-tuning code ready - uncomment when you have training data and GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb2923",
   "metadata": {},
   "source": [
    "## 5. Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Get token from environment or prompt\n",
    "hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "else:\n",
    "    print(\"No HF token found. Set HF_TOKEN in .env or run:\")\n",
    "    print(\"  from huggingface_hub import login\")\n",
    "    print(\"  login()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push dataset to Hub (optional)\n",
    "'''\n",
    "if dataset:\n",
    "    dataset.push_to_hub(\n",
    "        \"Elizo/proves-cubesat-dependencies\",\n",
    "        private=True  # Set to False to share publicly\n",
    "    )\n",
    "    print(\"Dataset pushed to Hugging Face Hub!\")\n",
    "'''\n",
    "\n",
    "# Push fine-tuned model to Hub (after training)\n",
    "'''\n",
    "model.push_to_hub(\n",
    "    \"Elizo/proves-dependency-extractor-lora\",\n",
    "    token=hf_token,\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    \"Elizo/proves-dependency-extractor-lora\",\n",
    "    token=hf_token,\n",
    ")\n",
    "print(\"Model pushed to Hugging Face Hub!\")\n",
    "'''\n",
    "\n",
    "print(\"Push code ready - uncomment after training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595aff0",
   "metadata": {},
   "source": [
    "## 6. Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference (after training)\n",
    "'''\n",
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"### Instruction:\n",
    "Extract dependencies from the following documentation and assess criticality.\n",
    "\n",
    "### Input:\n",
    "The ImuManager component initializes during boot and requires the LinuxI2cDriver\n",
    "to communicate with the BMX160 IMU sensor. If I2C fails, the spacecraft cannot\n",
    "determine its orientation for pointing solar panels at the sun.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "\n",
    "print(\"Inference code ready - uncomment after training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac69694",
   "metadata": {},
   "source": [
    "## ğŸ“Š Training Data Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CURATOR AGENT (Claude)                                     â”‚\n",
    "â”‚     â”‚                                                       â”‚\n",
    "â”‚     â–¼                                                       â”‚\n",
    "â”‚  Extracts dependencies from docs                            â”‚\n",
    "â”‚     â”‚                                                       â”‚\n",
    "â”‚     â–¼                                                       â”‚\n",
    "â”‚  HITL: Human reviews HIGH criticality                       â”‚\n",
    "â”‚     â”‚                                                       â”‚\n",
    "â”‚     â”œâ”€[Approve]â”€â”€â–º training_interactions (confirmed)        â”‚\n",
    "â”‚     â”œâ”€[Reject]â”€â”€â”€â–º training_interactions (negative)         â”‚\n",
    "â”‚     â””â”€[CORRECT]â”€â”€â–º training_interactions (GOLD!) â—„â”€â”€ BEST   â”‚\n",
    "â”‚                           â”‚                                 â”‚\n",
    "â”‚                           â–¼                                 â”‚\n",
    "â”‚                    training_examples                        â”‚\n",
    "â”‚                    (Alpaca format)                          â”‚\n",
    "â”‚                           â”‚                                 â”‚\n",
    "â”‚                           â–¼                                 â”‚\n",
    "â”‚                    This notebook exports                    â”‚\n",
    "â”‚                           â”‚                                 â”‚\n",
    "â”‚                           â–¼                                 â”‚\n",
    "â”‚                    Fine-tune local LLM                      â”‚\n",
    "â”‚                    (Llama/Mistral/Qwen)                     â”‚\n",
    "â”‚                           â”‚                                 â”‚\n",
    "â”‚                           â–¼                                 â”‚\n",
    "â”‚                    Push to Hugging Face                     â”‚\n",
    "â”‚                    (your own model!)                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Collect more training data** - Run the curator agent and provide corrections\n",
    "2. **Get GPU access** - Use Colab, Lambda Labs, or your own GPU\n",
    "3. **Fine-tune** - Start with a small model like Llama-3.2-1B\n",
    "4. **Evaluate** - Test on held-out examples\n",
    "5. **Iterate** - Collect more data, retrain, improve"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
